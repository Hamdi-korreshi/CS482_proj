{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow in ./bruh/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./bruh/lib/python3.10/site-packages (from tensorflow) (1.26.2)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./bruh/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./bruh/lib/python3.10/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./bruh/lib/python3.10/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./bruh/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./bruh/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./bruh/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: setuptools in ./bruh/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./bruh/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./bruh/lib/python3.10/site-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./bruh/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./bruh/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./bruh/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./bruh/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./bruh/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./bruh/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./bruh/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./bruh/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./bruh/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./bruh/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./bruh/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow_hub in ./bruh/lib/python3.10/site-packages (0.15.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in ./bruh/lib/python3.10/site-packages (from tensorflow_hub) (4.23.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./bruh/lib/python3.10/site-packages (from tensorflow_hub) (1.26.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow_text in ./bruh/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.13.0 in ./bruh/lib/python3.10/site-packages (from tensorflow_text) (0.15.0)\n",
      "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in ./bruh/lib/python3.10/site-packages (from tensorflow_text) (2.15.0.post1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (23.5.26)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.14.1)\n",
      "Requirement already satisfied: setuptools in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (4.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.60.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.4.0)\n",
      "Requirement already satisfied: packaging in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (23.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.26.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.34.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (4.23.4)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.0.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./bruh/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./bruh/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.42.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.7.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.25.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.5.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./bruh/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./bruh/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./bruh/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./bruh/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./bruh/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./bruh/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./bruh/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./bruh/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./bruh/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.2.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow_addons in ./bruh/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./bruh/lib/python3.10/site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in ./bruh/lib/python3.10/site-packages (from tensorflow_addons) (23.2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "!pip install tensorflow_hub\n",
    "import tensorflow_hub as hub\n",
    "!pip install tensorflow_text\n",
    "!pip install tensorflow_addons\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppressing tf.hub warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 8092\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "annotations_file = \"flickr8k/captions.txt\"\n",
    "images_dir = \"flickr8k/Images\"\n",
    "\n",
    "with open(annotations_file, \"r\") as f:\n",
    "    annotations = f.readlines()\n",
    "\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for element in annotations:\n",
    "    caption = f\"{element.lower().rstrip(',')}\"\n",
    "    image_id = element.split(\".\")[0]\n",
    "    image_path = os.path.join(images_dir, image_id + \".jpg\")\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(f\"Number of images: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:06<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 training examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 26.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2070 evaluation examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = 3000\n",
    "valid_size = 1000\n",
    "captions_per_image = 2\n",
    "images_per_file = 45\n",
    "tfrecords_dir = \"/content/drive/MyDrive/Colab Notebooks/Tf_records\"\n",
    "\n",
    "image_paths.remove('flickr8k/Images/image,caption\\n.jpg')\n",
    "# image_paths.remove(\"/content/drive/MyDrive/Colab Notebooks/archive.zip (Unzipped Files)/Images/378453580_21d688748e.jpg\")\n",
    "train_image_paths = image_paths[:train_size]\n",
    "num_train_files = int(np.ceil(train_size / images_per_file))\n",
    "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
    "valid_image_paths = image_paths[:-valid_size]\n",
    "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
    "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(tfrecords_dir)\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = images_per_file * file_idx\n",
    "        end_idx = start_idx + images_per_file\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, batch_size):\n",
    "\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(batch_size * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained Xception model to be used as the base encoder.\n",
    "    xception = keras.applications.Xception(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Receive the images as inputs.\n",
    "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocess the input image.\n",
    "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate the embeddings for the images using the xception model.\n",
    "    embeddings = xception(xception_input)\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the BERT preprocessing module.\n",
    "    preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",name=\"text_preprocessing\",)\n",
    "    # Load the pre-trained BERT model to be used as the base encoder.\n",
    "    bert = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",True,)\n",
    "    # Set the trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "    # Receive the text as inputs.\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess the text.\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoder(keras.Model):\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        # Place each encoder on a separate GPU (if available).\n",
    "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Get the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Get the embeddings for the images.\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Monitor loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15  # In practice, train for at least 30 epochs\n",
    "batch_size = 32\n",
    "\n",
    "vision_encoder = create_vision_encoder(num_projection_layers=1, projection_dims=32, dropout_rate=0.1)\n",
    "text_encoder = create_text_encoder(num_projection_layers=1, projection_dims=32, dropout_rate=0.1)\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Steps per epoch: 188\n",
      "Epoch 1/15\n",
      "    188/Unknown - 55s 262ms/step - loss: 11.5960"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 20:53:10.259009: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3723571723331092524\n",
      "2023-12-15 20:53:10.259055: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13532719841379844500\n",
      "2023-12-15 20:53:10.259063: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2218986579264143704\n",
      "2023-12-15 20:53:10.259076: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2632200291182904354\n",
      "2023-12-15 20:53:10.259081: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13084734711136828146\n",
      "2023-12-15 20:53:10.259110: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8854044631162097867\n",
      "2023-12-15 20:53:10.259126: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16498547626910302463\n",
      "2023-12-15 20:53:10.259130: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10135740481069327886\n",
      "2023-12-15 20:53:10.259152: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10216170932789928643\n",
      "2023-12-15 20:53:10.259159: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7886224125523299392\n",
      "2023-12-15 20:53:10.259183: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18092299504195161860\n",
      "2023-12-15 20:53:10.259203: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5439921578701172900\n",
      "2023-12-15 20:53:10.259217: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 956083178086519452\n",
      "2023-12-15 20:53:10.259230: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5273932365774840781\n",
      "2023-12-15 20:53:10.259249: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12675395804627678028\n",
      "2023-12-15 20:53:10.259264: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15181652456703498016\n",
      "2023-12-15 20:53:10.259277: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6516413968609882923\n",
      "2023-12-15 20:53:10.259300: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 491846808377189669\n",
      "2023-12-15 20:53:10.259306: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10900133960519392188\n",
      "2023-12-15 20:53:10.259340: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13893648678966052519\n",
      "2023-12-15 20:53:10.259361: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18060975654561449915\n",
      "2023-12-15 20:53:10.259380: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8150223072427292055\n",
      "2023-12-15 20:53:10.259399: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8961880885838865561\n",
      "2023-12-15 20:53:10.259419: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14103934030604336900\n",
      "2023-12-15 20:53:10.259447: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12186262360098433739\n",
      "2023-12-15 20:53:10.259471: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13088328994410131159\n",
      "2023-12-15 20:53:27.915696: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16336288609927701686\n",
      "2023-12-15 20:53:27.915745: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1845987295078704604\n",
      "2023-12-15 20:53:27.915756: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17867160851166795290\n",
      "2023-12-15 20:53:27.915763: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7747016035629790308\n",
      "2023-12-15 20:53:27.915792: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8603261068914972350\n",
      "2023-12-15 20:53:27.915817: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7058116132614385775\n",
      "2023-12-15 20:53:27.915846: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2992867411849263836\n",
      "2023-12-15 20:53:27.915869: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4577413007631303167\n",
      "2023-12-15 20:53:27.915889: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5640610904334113806\n",
      "2023-12-15 20:53:27.915907: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5959776690053594683\n",
      "2023-12-15 20:53:27.915931: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10584589775796465622\n",
      "2023-12-15 20:53:27.915956: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10675185496253957385\n",
      "2023-12-15 20:53:27.915966: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6742394234218419835\n",
      "2023-12-15 20:53:27.915969: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1327590598562659945\n",
      "2023-12-15 20:53:27.915973: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9621347562567403923\n",
      "2023-12-15 20:53:27.915976: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3492928657362856237\n",
      "2023-12-15 20:53:27.915980: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10800834827247954125\n",
      "2023-12-15 20:53:27.915983: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12285636551731670341\n",
      "2023-12-15 20:53:27.915987: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10394967791755878125\n",
      "2023-12-15 20:53:27.916004: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5049829400712986569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 73s 358ms/step - loss: 11.5960 - val_loss: 3.3998 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "188/188 [==============================] - 66s 352ms/step - loss: 3.2310 - val_loss: 2.8665 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 2.6305 - val_loss: 2.3338 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 2.2360 - val_loss: 1.9218 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "188/188 [==============================] - 67s 355ms/step - loss: 1.8701 - val_loss: 1.6291 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "188/188 [==============================] - 67s 356ms/step - loss: 1.6448 - val_loss: 1.3790 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 1.4690 - val_loss: 1.4268 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 1.3362 - val_loss: 1.1073 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "188/188 [==============================] - 67s 355ms/step - loss: 1.1967 - val_loss: 1.0669 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "188/188 [==============================] - 67s 356ms/step - loss: 1.1366 - val_loss: 0.9669 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "188/188 [==============================] - 67s 356ms/step - loss: 1.0353 - val_loss: 0.9321 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 1.0418 - val_loss: 0.9771 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 0.9583 - val_loss: 0.7418 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 0.9510 - val_loss: 0.8044 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "188/188 [==============================] - 67s 355ms/step - loss: 0.9382 - val_loss: 0.9074 - lr: 0.0010\n",
      "Training completed. Saving vision and text encoders...\n",
      "Models are saved.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
    "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
    "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "print(\"Training completed. Saving vision and text encoders...\")\n",
    "vision_encoder.save(\"vision_encoder\")\n",
    "text_encoder.save(\"text_encoder\")\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bruh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
